# my-assistant-cli
Docker image to run an assistant CLI to interact with a dockerized LLM. You can find a list of available LLMs [here](https://hub.docker.com/repositories/ilopezluna)

## Requirements
- Docker
- NVIDIA for GPU support 

## Usage

```bash
docker run -it --rm -v /var/run/docker.sock:/var/run/docker.sock ilopezluna/my-assistant-cli  "ilopezluna/thebloke_tinyllama-1_1b-chat-v1_0-gguf:tinyllama-1.1b-chat-v1.0.Q4_K_S.gguf_ollama_0.2.1" "What is the capital of Spain?"
```

You can use bash commands to enrich the prompt, for example:

```bash
docker run -it --rm -v /var/run/docker.sock:/var/run/docker.sock ilopezluna/my-assistant-cli:nodejs  "ilopezluna/ollama
-llama3.1:0.2.8-8b" "I will provide you the contents of the files of a folder, your task is to describe what this folder is about. Here is the content: '$(for file in src/*; do [[ -f "$file" ]] && echo -e "\nContents of $file:" && cat "$file"; done 
)'
"

Based on the provided code, this folder appears to be a simple command-line application that uses the Testcontainers library to run an instance of the Llama (a large language model) container and then generates a response to a given prompt.

Here's a high-level overview of what the application does:

1. It takes two arguments: an image name and a prompt.
2. It starts a new instance of the Llama container using Testcontainers, with the specified image name. The container is run in a reusable mode to allow for subsequent executions without having to start from scratch each time.
3. Once the container is running, it generates a response to the provided prompt by making an HTTP POST request to the /api/generate endpoint of the Llama API within the container.
4. The response generated by the Llama model is then printed to the console.

The application uses various libraries and tools, including:

* Testcontainers: for running the Llama container
* Go's net/http package: for making HTTP requests to the Llama API
* Go's encoding/json package: for serializing and deserializing JSON data

Overall, this folder appears to be a simple command-line application that uses the Llama model to generate responses to user prompts.
```
Note: I'm using `:nodejs` tag to use the NodeJS version of the image, because reuse flag seems not having any effect in Testcontainers Go version.


